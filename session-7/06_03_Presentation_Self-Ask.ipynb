{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTo96wd2ogSndqeNlyDXFl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RG079okejf7r"},"outputs":[],"source":["%%capture\n","!pip install langchain openai langchainhub duckduckgo-search"]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-u2S9UzlVu8","executionInfo":{"status":"ok","timestamp":1697025246699,"user_tz":300,"elapsed":8183,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"7d97e1ba-8acb-48e5-fb56-fb8d3b7293b9"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Your OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"]}]},{"cell_type":"markdown","source":["# Self-Ask\n","\n","This prompting method was introduced in the October 2022 paper titled \"[Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/pdf/2210.03350.pdf)\" by Ofir Press, et. al.\n","\n","### üß† **Compositional Reasoning in Language Models**\n","\n","- Focuses on solving complex problems by piecing together answers to smaller sub-problems.\n","\n","### üìè **Introducing the 'Compositionality Gap'**\n","\n","- A new metric to evaluate how models solve sub-problems but fail to integrate these solutions into a comprehensive answer.\n","\n","### üìà **Findings on Model Performance**\n","\n","- Larger models like GPT-3 improve more on single-hop (simple) questions than multi-hop (complex) ones.\n","\n","- This indicates the 'compositionality gap' doesn't shrink with bigger models.\n","\n","### üîç **Challenge with Bigger Models**\n","\n","- Despite remembering and recalling more facts, these models struggle with integrating these facts for complex problem-solving.\n","\n","# ü§ñ **Self-Ask Prompting Technique**\n","\n","<figure>\n","  <img src=\"https://ar5iv.labs.arxiv.org/html/2210.03350/assets/x5.png\" alt=\"Image Description\" style=\"width:100%\">\n","  <figcaption>Direct prompting (Brown et al., 2020) compared to chain of thought and self-ask method on a question from Bamboogle. Text with a white background is the prompt, text with a green background is the LM output, and underlined text is the inference-time question</figcaption>\n","  <a href=\"https://ar5iv.labs.arxiv.org/html/2210.03350\">Image Source</a>\n","</figure>\n","\n","\n","### üîç **How it Works**\n","\n","1. **Generate Follow-Up Questions**: Instead of directly answering, the model first creates related, simpler questions.\n","\n","2. **Answer the Follow-Ups**: The model then resolves these self-generated questions.\n","\n","3. **Answer the Original Question**: Finally, it combines the answers to respond to the initial, complex question.\n","\n","### üåü **Benefits of Self-Ask Technique**\n","\n","- Promotes explicit reasoning by breaking down complex questions.\n","\n","- Structured approach suitable for integrating with a search engine for improved accuracy.\n","\n","- It's like the model having a conversation with itself to better tackle a problem.\n","\n","### ‚úÖ **Impact**\n","\n","- Simplifies complex problems into manageable parts for a comprehensive final answer.\n","\n","\n","\n","# üîç**Understanding the Self-Ask Pseudo-Code**\n","\n","You can find the original implementation of Self-Ask from the [paper's GitHub repo](https://github.com/ofirpress/self-ask/blob/main/self-ask_plus_search-engine_demo.ipynb).\n","\n","\n","### üîë **Key Components of the Code**\n","\n","1. **`self_ask` Function**: The core function that handles the self-ask process.\n","   - Starts with an initial question.\n","   - Continuously generates and answers follow-up questions using an external source.\n","   - Ends with a final answer after resolving all follow-ups.\n","\n","2. **Helper Functions**:\n","   - `frame_question`: Prepares the initial question for processing.\n","   - `call_gpt`: Simulates a call to the GPT model with the current prompt.\n","   - `get_last_line`: Extracts the last line of the text, often where follow-up questions or answers are found.\n","   - `extract_question`: Pulls out a follow-up question from the text.\n","   - `get_answer`: Represents fetching an answer from an external source.\n","\n","### üîÑ **Workflow of the Algorithm**\n","\n","- Begin with framing the initial question.\n","- Enter a loop where:\n","  - If a follow-up question is detected, an external answer is sought.\n","  - This external answer is added to the current prompt, and the model is called again.\n","  - If there's no external answer, it proceeds to seek the final answer.\n","- The process repeats until a final answer is formulated.\n","\n","\n","\n","```python\n","\n","def self_ask(question: str) -> str:\n","    cur_prompt = frame_question(question)  \n","    ret_text = call_gpt(cur_prompt)\n","\n","    while \"Follow up:\" in get_last_line(ret_text):\n","        extracted_question = extract_question(ret_text)\n","\n","        external_answer = get_answer(extracted_question)\n","\n","        if external_answer:\n","            cur_prompt += \"Intermediate answer: \" + external_answer\n","            ret_text = call_gpt(cur_prompt)\n","        else:\n","            cur_prompt += \"Intermediate answer: \"\n","            ret_text = call_gpt(cur_prompt + \" Follow up: \" + \"Final answer: \")\n","\n","    if \"Final answer:\" not in ret_text:\n","        cur_prompt += \"Final answer: \"\n","        ret_text = call_gpt(cur_prompt)\n","\n","    return cur_prompt + ret_text\n","\n","def frame_question(question: str) -> str:\n","    return question\n","\n","def call_gpt(prompt: str) -> str:\n","    return \"\"\n","\n","def get_last_line(text: str) -> str:\n","    return text.split('\\n')[-1] if '\\n' in text else text\n","\n","def extract_question(text: str) -> str:\n","\n","def get_answer(question: str) -> str:\n","    return \"\"\n","```\n","\n","# LangChain makes Self-Ask easy"],"metadata":{"id":"U5hJoozgliRN"}},{"cell_type":"code","source":["from langchain import hub\n","from langchain.llms import OpenAI\n","from langchain.agents import initialize_agent, Tool\n","from langchain.agents import AgentExecutor, AgentType\n","from langchain.agents.output_parsers import SelfAskOutputParser\n","from langchain.agents.format_scratchpad import format_log_to_str\n","from langchain.utilities import DuckDuckGoSearchAPIWrapper\n","\n","llm = OpenAI(temperature=0)\n","search = DuckDuckGoSearchAPIWrapper()"],"metadata":{"id":"WvZ6C_Qnou3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tools = [\n","    Tool(\n","        name=\"Intermediate Answer\",\n","        func=search.run,\n","        description=\"useful for when you need to ask with search\",\n","    )\n","]"],"metadata":{"id":"SudF-2zSplNK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = hub.pull(\"hwchase17/self-ask-with-search\")"],"metadata":{"id":"d7tGCsJno_aj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(prompt.template)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b1dtjPHepBII","executionInfo":{"status":"ok","timestamp":1697027507884,"user_tz":300,"elapsed":2,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"4ee1ed6b-e0ac-4cf6-d5ec-8eb561ca0395"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Question: Who lived longer, Muhammad Ali or Alan Turing?\n","Are follow up questions needed here: Yes.\n","Follow up: How old was Muhammad Ali when he died?\n","Intermediate answer: Muhammad Ali was 74 years old when he died.\n","Follow up: How old was Alan Turing when he died?\n","Intermediate answer: Alan Turing was 41 years old when he died.\n","So the final answer is: Muhammad Ali\n","\n","Question: When was the founder of craigslist born?\n","Are follow up questions needed here: Yes.\n","Follow up: Who was the founder of craigslist?\n","Intermediate answer: Craigslist was founded by Craig Newmark.\n","Follow up: When was Craig Newmark born?\n","Intermediate answer: Craig Newmark was born on December 6, 1952.\n","So the final answer is: December 6, 1952\n","\n","Question: Who was the maternal grandfather of George Washington?\n","Are follow up questions needed here: Yes.\n","Follow up: Who was the mother of George Washington?\n","Intermediate answer: The mother of George Washington was Mary Ball Washington.\n","Follow up: Who was the father of Mary Ball Washington?\n","Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n","So the final answer is: Joseph Ball\n","\n","Question: Are both the directors of Jaws and Casino Royale from the same country?\n","Are follow up questions needed here: Yes.\n","Follow up: Who is the director of Jaws?\n","Intermediate answer: The director of Jaws is Steven Spielberg.\n","Follow up: Where is Steven Spielberg from?\n","Intermediate answer: The United States.\n","Follow up: Who is the director of Casino Royale?\n","Intermediate answer: The director of Casino Royale is Martin Campbell.\n","Follow up: Where is Martin Campbell from?\n","Intermediate answer: New Zealand.\n","So the final answer is: No\n","\n","Question: {input}\n","Are followup questions needed here:{agent_scratchpad}\n"]}]},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","deep_learning_template = \"\"\"\n","Question: Which architecture is more suited for image classification, LSTM or CNN?\n","Are follow-up questions needed here: Yes.\n","Follow-up 1: What is the primary application of LSTM?\n","Intermediate Answer: LSTM (Long Short-Term Memory) is primarily used for sequential data and time series analysis.\n","Follow-up 2: What is the primary application of CNN?\n","Intermediate Answer: CNN (Convolutional Neural Network) is primarily used for image and video recognition.\n","Final Answer: CNN is more suited for image classification.\n","\n","Question: Which optimizer converges faster, SGD or Adam?\n","Are follow-up questions needed here: Yes.\n","Follow-up 1: What is the main advantage of SGD (Stochastic Gradient Descent)?\n","Intermediate Answer: SGD is simple and easy to implement.\n","Follow-up 2: What is the main advantage of Adam optimizer?\n","Intermediate Answer: Adam combines the benefits of both AdaGrad and RMSProp and usually converges faster than SGD.\n","Final Answer: Adam converges faster.\n","\n","Question: For natural language processing tasks, which dataset is larger, MNIST or WikiText-103?\n","Are follow-up questions needed here: Yes.\n","Follow-up 1: What kind of data does the MNIST dataset contain?\n","Intermediate Answer: MNIST contains handwritten digits and is used for image classification.\n","Follow-up 2: What kind of data does the WikiText-103 dataset contain?\n","Intermediate Answer: WikiText-103 is a large language modeling dataset containing text from Wikipedia articles.\n","Final Answer: WikiText-103 is larger and is used for natural language processing tasks.\n","\n","Question: {input}\n","Are followup questions needed here:{agent_scratchpad}\n","\"\"\"\n","\n","deep_learning_prompt = PromptTemplate.from_template(deep_learning_template)"],"metadata":{"id":"IUet1sBJtFXV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîß **Binding a Stop Sequence in LLM Generation**\n","\n","- The concept involves using a specific string (in this case, `\"\\nIntermediate answer:\"`) as a signal for the language model (LLM) to halt text generation.\n","\n","### üéØ **Purpose in Self-Ask with Search Agent**\n","\n","- Facilitates a structured dialogue between the LLM and the search agent.\n","\n","- The LLM stops after providing an intermediate answer, allowing the search agent to process this information and pose a follow-up question.\n","\n","### üõë **How the Stop Sequence Works**\n","\n","- Acts as a delimiter in the LLM's output.\n","\n","- Once this sequence is generated, it indicates the end of the current response phase and triggers subsequent processing or querying steps.\n","\n","%%html\n","<figure>\n","  <img src=\"https://ar5iv.labs.arxiv.org/html/2210.03350/assets/x9.png\" alt=\"Image Description\" style=\"width:100%\">\n","  <figcaption>Self-ask + Search Engine: prompt on white background, LM-generated text in green. Start by using a few-shot prompt and appending the test-time question (underlined) to it. The LM then generates a follow-up question which inputs to an internet search engine. The response is inserted back into the rest of the prompt to let the LM generate the next follow-up question. This process then repeats until the LM decides to output the final answer.</figcaption>\n","  <a href=\"https://ar5iv.labs.arxiv.org/html/2210.03350\">Image Source</a>\n","</figure>\n"],"metadata":{"id":"hY0SaVqIqoLC"}},{"cell_type":"code","source":["llm_with_stop = llm.bind(stop=[\"\\nIntermediate answer:\"])"],"metadata":{"id":"8W6A1MMuparM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent = {\n","    \"input\": lambda x: x[\"input\"],\n","    # Use some custom observation_prefix/llm_prefix for formatting\n","    \"agent_scratchpad\": lambda x: format_log_to_str(\n","        x['intermediate_steps'],\n","        observation_prefix=\"\\nIntermediate answer: \",\n","        llm_prefix=\"\",\n","    ),\n","} | prompt | llm_with_stop | SelfAskOutputParser()"],"metadata":{"id":"0TMtTl-6pgOr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent_executor = AgentExecutor(agent=agent,\n","                               tools=tools,\n","                               verbose=True,\n","                               handle_parsing_errors=True\n","                               )"],"metadata":{"id":"gbZM6mZOtk2L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent_executor.invoke({\"input\": \"How do attention mechanisms in transformer architectures, like BERT and GPT, address the limitations of recurrent neural networks (RNNs) in processing long sequences, and what implications does this have for tasks like machine translation and document summarization?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8uQ98KvtxEt","executionInfo":{"status":"ok","timestamp":1697027803715,"user_tz":300,"elapsed":5222,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"c1e4135c-96fb-4654-f738-26b56a038964"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m Yes.\n","Follow up: What are the limitations of recurrent neural networks (RNNs) in processing long sequences?\u001b[0m\u001b[36;1m\u001b[1;3mWhat are some of the challenges and limitations of RNNs? Powered by AI and the LinkedIn community Recurrent neural networks (RNNs) are a type of artificial neural network (ANN) that... Published: 08 Aug 2023 To set realistic expectations for AI without missing opportunities, it's important to understand both the capabilities and limitations of different model types. Two categories of algorithm s that have propelled the field of AI forward are convolutional neural networks ( CNNs) and recurrent neural networks ( RNNs ). Implementing Recurrent Neural Networks (RNNs) within generative modeling might seem complex, but with a step-by-step guide, novices can confidently navigate the process. Begin by understanding the core concepts of RNNs, such as the recurrent loops and hidden states. Preprocessing the data and choosing appropriate hyperparameters follow suit. Recently, recurrent neural networks (RNNs) have become a popular machine learning tool for studying the non-linear dynamics of neural and behavioural processes by emulating an underlying system of ... Recurrent neural networks (RNNs) are neural network architectures with hidden state and which use feedback loops to process a sequence of data that ultimately informs the final output. Therefore, RNN models can recognize sequential characteristics in the data and help to predict the next likely data point in the data sequence.\u001b[0m\u001b[32;1m\u001b[1;3m\n","So the final answer is: RNNs have difficulty in capturing long-term dependencies due to the vanishing gradient problem, which limits their ability to process long sequences of data. Attention mechanisms in transformer architectures, like BERT and GPT, address this limitation by allowing the model to focus on specific parts of the input sequence, which enables them to better capture long-term dependencies and improve performance on tasks like machine translation and document summarization.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input': 'How do attention mechanisms in transformer architectures, like BERT and GPT, address the limitations of recurrent neural networks (RNNs) in processing long sequences, and what implications does this have for tasks like machine translation and document summarization?',\n"," 'output': 'RNNs have difficulty in capturing long-term dependencies due to the vanishing gradient problem, which limits their ability to process long sequences of data. Attention mechanisms in transformer architectures, like BERT and GPT, address this limitation by allowing the model to focus on specific parts of the input sequence, which enables them to better capture long-term dependencies and improve performance on tasks like machine translation and document summarization.'}"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["agent_executor.invoke({\"input\": \"How does the qLoRA approach help in fine-tuning LLaMA2 models without catastrophic forgetting?\"})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-La7Dlgt4FV","executionInfo":{"status":"ok","timestamp":1697027935102,"user_tz":300,"elapsed":7135,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"73231982-f5a2-4ca1-c8d9-d294e1e4729a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m Yes.\n","Follow up: What is the qLoRA approach?\u001b[0m\u001b[36;1m\u001b[1;3mQLoRA: Here, you first quantize the LLM and then perform LoRa training. That's it. ... In this approach, only the LoRa adapters are stored in FP32 format, while the rest remain in 4-bit format ... QLoRA, short for \"Quantized Layerwise Resource Allocation,\" is an innovative approach to training large deep learning models more efficiently. It addresses the resource limitations that arise... Understanding QLoRA (Quantized LoRA) Optimizations that QLoRA brings What is fine-tuning? Fine-tuning is adapting pre-trained language models to specific tasks and domains. It complements... LoRA is an orthogonal fine-tuning method and can be combined with many prior methods such as prefix-tuning. LoRA Method Description Here, we briefly describe the LoRA method. Let us denote the... The QLoRA approach is poised to serve as a catalyst for a new era of innovation within the open-source community. Developers can now create better performing models that can be fine-tuned very...\u001b[0m\u001b[32;1m\u001b[1;3m\n","Follow up: How does the qLoRA approach help in fine-tuning LLaMA2 models without catastrophic forgetting?\u001b[0m\u001b[36;1m\u001b[1;3mLeveraging qLoRA for Fine-Tuning of Task-Fine-Tuned Models Without Catastrophic Forgetting: A Case Study with LLaMA2 (-chat) | by Aris Tsakpinis | Sep, 2023 | Towards Data Science. Learn how to infuse knowledge into purpose-fine-tuned models while keeping their task-specific nature. Aris Tsakpinis. ¬∑. Follow. Published in. Towards Data Science. ¬∑. It boasts an augmented context length and the innovative introduction of Grouped Query Attention (GQA) which enhances the inference scalability of the model. This guide will walk you through the process of fine-tuning Llama 2 with LoRA for Question Answering tasks. Why Use Llama 2. QLORA: Efficient Finetuning of Quantized LLMs. Gpt-4's bullet points for the abstract: - QLoRA: Efficient finetuning approach that reduces memory usage for finetuning a 65B parameter model on a single 48GB GPU while maintaining full 16-bit finetuning task performance. By Artur Niederfahrenhorst, Kourosh Hakhamaneshi and Rehaan Ahmad | September 6, 2023. In this blog, we compare full-parameter fine-tuning with LoRA and answer questions around the strengths and weaknesses of the two techniques. We train the Llama 2 models on the same three real-world use cases as in our previous blog post. To suit every text generation needed and fine-tune these models, we will use QLoRA (Efficient Finetuning of Quantized LLMs), a highly efficient fine-tuning technique that involves quantizing a pretrained LLM to just 4 bits and adding small \"Low-Rank Adapters\". This unique approach allows for fine-tuning LLMs using just a single GPU!\u001b[0m\u001b[32;1m\u001b[1;3m\n","So the final answer is: The qLoRA approach reduces memory usage for fine-tuning a 65B parameter model on a single 48GB GPU while maintaining full 16-bit finetuning task performance. It involves quantizing a pretrained LLM to just 4 bits and adding small \"Low-Rank Adapters\".\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input': 'How does the qLoRA approach help in fine-tuning LLaMA2 models without catastrophic forgetting?',\n"," 'output': 'The qLoRA approach reduces memory usage for fine-tuning a 65B parameter model on a single 48GB GPU while maintaining full 16-bit finetuning task performance. It involves quantizing a pretrained LLM to just 4 bits and adding small \"Low-Rank Adapters\".'}"]},"metadata":{},"execution_count":55}]}]}