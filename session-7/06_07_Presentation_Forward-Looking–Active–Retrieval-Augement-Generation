{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1JDuyfKY67dnYM5IUlxZgUA0m4u7cFdst","timestamp":1697211613393}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5NpLUx0bgw57"},"outputs":[],"source":["%%capture\n","!pip install langchain openai faiss-cpu arxiv pymupdf tiktoken"]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sHgv4EQ7hBxA","executionInfo":{"status":"ok","timestamp":1696422210712,"user_tz":300,"elapsed":3907,"user":{"displayName":"Harpreet Sahota","userId":"15663196278719964515"}},"outputId":"0e2397c9-2bd8-4b30-8c78-0c23257cb588"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Your OpenAI API Key:路路路路路路路路路路\n"]}]},{"cell_type":"markdown","source":["# FLARE\n","\n","### What is FLARE?\n","\n","[FLARE](https://arxiv.org/pdf/2305.06983.pdf) stands for `Forward-Looking Active REtrieval` augmented generation.  This is a method that was introduced in 2023 in a paper titled \"Active Retrieval Augmented Generation\" by Zhengbao Jiang, et al. The original implementation can be found in [the paper's GitHub repo](https://github.com/jzbjyb/FLARE), which the LangChain implementation draws heavy inspiration from.\n","\n","\n","##  **Why FLARE Matters**\n","\n","- Enhances large language models by fetching info from external sources while generating content.\n","\n","- Tackles the issue of models making stuff up or being factually wrong.\n","\n","- Aims for outputs that are more accurate and well-informed.\n","\n","##  **FLARE's Approach:**\n","- Starts by answering a question.\n","\n","- Looks up relevant documents when hitting \"uncertain\" tokens (based on log probs).\n","\n","- Uses these documents to keep generating.\n","\n","- Repeats the process until done.\n","\n","- Highlights uncertain tokens and forms questions for LLM to find answers.\n","\n","##  **How FLARE Stands Out from Traditional RAG:**\n","\n","- Traditional RAG fetches info once based on the input, then generates an output.\n","\n","- Can be restrictive for long texts.\n","\n","- FLARE keeps deciding **when and what to fetch** during the generation.\n","\n","- Predicts the upcoming sentence to guide future content queries.\n","\n","- If a sentence has low-confidence tokens, it fetches relevant info and rewrites the sentence.\n","\n","\n","<img src=\"https://raw.githubusercontent.com/jzbjyb/FLARE/main/res/flare.gif\">\n","\n","\n","\n","###  **FLARE Pipeline Steps:**\n","\n","1. **Step 1**: Kick off with user input and initial info gathered.\n","\n","2. **Step 2**: Create a draft of the next sentence and spot any low-probability tokens.\n","\n","3. **Step 3**: If there are doubtful tokens, pull in relevant documents and refine that sentence. Keep iterating this until it's all wrapped up!\n","\n","###  **FLARE Prompt Structure:**\n","\n","- Guides language models to create search queries during answer generation.\n","\n","- For example: \"The colors on the flag of Ghana mean... Red stands for [Search(Ghana flag red meaning)] the blood of martyrs...\"\n","\n","###  **Direct FLARE Method:**\n","\n","- Uses the next sentence generated by the language model for retrieval decisions.\n","\n","- Accepts the sentence if the model is confident.\n","\n","- If unsure, transforms the sentence into search queries to get more info, then revises the sentence.\n","\n","###  **Setting Up the FLARE Chain:**\n","\n","1. **LLM for Answers**: Needs to return log probabilities to spot uncertain tokens. Recommended: OpenAI wrapper (not ChatOpenAI wrapper, as it doesn't return logprobs).\n","\n","2. **LLM for Hypothetical Questions**: Can be any model. In this case, we use ChatOpenAI for its speed and cost-effectiveness.\n","\n","3. **Retriever**: Any retriever works. Here, the SERPER search engine is used for cost efficiency.\n","\n","####  **Key Parameters in FLARE:**\n","\n","1. **`max_generation_len`**: Caps the number of tokens generated before checking for uncertainties.\n","\n","2. **`min_prob`**: Tokens generated with a probability below this threshold are marked as uncertain."],"metadata":{"id":"BvaIosUctu5H"}},{"cell_type":"code","source":["import langchain\n","from langchain.document_loaders import ArxivLoader\n","from langchain.vectorstores import FAISS\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.llms import OpenAI\n","from langchain.chains import FlareChain\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# Uncomment the below if you want to see all the intermediate steps\n","# langchain.verbose=True"],"metadata":{"id":"mEoXA-kQpdGz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Same pattern as we saw in the RAG lecture."],"metadata":{"id":"KoNiUTL8ahup"}},{"cell_type":"code","source":["# instantiate llm\n","llm = OpenAI()\n","\n","#instantiate embeddings model\n","embeddings = OpenAIEmbeddings()\n","\n","# fetch docs from arxiv, in this case it's the FLARE paper\n","docs = ArxivLoader(query=\"2305.06983\", load_max_docs=2).load()\n","\n","# instantiate text splitter\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n","\n","# split the document into chunks\n","doc_chunks = text_splitter.split_documents(docs)\n","\n","# create vector store consisting of embeddigs for document\n","vector_store = FAISS.from_documents(doc_chunks, embedding=embeddings)\n","vector_store_retriever = vector_store.as_retriever()"],"metadata":{"id":"QA126pbpu0UK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Set up the FLARE chain"],"metadata":{"id":"xtSrDBBSa0i5"}},{"cell_type":"code","source":["flare = FlareChain.from_llm(\n","    llm=llm,\n","    retriever=vector_store_retriever,\n","    max_generation_len=500,\n","    min_prob=0.45,\n","    verbose=True\n",")"],"metadata":{"id":"4DZGInhUwgp_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Start generation"],"metadata":{"id":"OE-f2-xma2q0"}},{"cell_type":"code","source":["flare.run(\"What is FLARE?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"DQ1oAcawL3RM","executionInfo":{"status":"ok","timestamp":1696432200575,"user_tz":300,"elapsed":3333,"user":{"displayName":"Harpreet Sahota","userId":"15663196278719964515"}},"outputId":"c610a4cb-c96a-4b1e-c355-29e94b06daa6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' FLARE is a generic retrieval-augmented generation method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["flare.run(\"What are the steps in a FLARE pipeline?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"KvaLunCFL4ug","executionInfo":{"status":"ok","timestamp":1696432205961,"user_tz":300,"elapsed":5387,"user":{"displayName":"Harpreet Sahota","userId":"15663196278719964515"}},"outputId":"ce4f8801-4d4f-44c8-a76a-ad7d3cbd789d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' The steps in a FLARE pipeline are as follows: \\n1. Generate a retrieval query based on the current context. \\n2. Retrieve relevant documents from an external knowledge resource. \\n3. Augment the generated sentence with the retrieved documents. \\n4. Iteratively generate the next sentence to gain insight into the future topic. \\n5. If uncertain tokens are present, retrieve relevant documents to regenerate the next sentence.\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["flare.run(\"What does a typical retrieval instruction look like in FLARE?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"a9K3HpCxMIJY","executionInfo":{"status":"ok","timestamp":1696432214874,"user_tz":300,"elapsed":8915,"user":{"displayName":"Harpreet Sahota","userId":"15663196278719964515"}},"outputId":"0949238b-3214-4c9d-e718-3c06ec6a32f0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' A typical retrieval instruction in FLARE is to generate \"[Search(query)]\" when additional information is needed (Schick et al., 2023). For example, \"The colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of martyrs, ...\". '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":66}]}]}