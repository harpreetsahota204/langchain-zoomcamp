{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNxvCQ6QSWe0QKqA18RX3eS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"r_LXpiKqjeNR"},"outputs":[],"source":["%%capture\n","!pip install langchain openai langchainhub duckduckgo-search wikipedia"]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vqPhjnvkk6oS","executionInfo":{"status":"ok","timestamp":1697194005620,"user_tz":300,"elapsed":8593,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"fd023cf6-d306-48e0-b1c6-171225aaca90"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Your OpenAI API Key:··········\n"]}]},{"cell_type":"markdown","source":["# **ReAct Prompting Framework Overview**\n","\n","The ReAct framework was [introduced in a 2022 paper, ReAct: Synergizing Reasoning and Acting in Language Models, by researchers at Google](https://blog.research.google/2022/11/react-synergizing-reasoning-and-acting.html).\n","\n","### 🌐 **ReAct (Reasoning + Acting) Explained**\n","\n","- Blends reasoning with action in language models.\n","\n","- Generates reasoning traces alongside task-specific actions.\n","\n","### 🔗 **Capabilities of ReAct**\n","\n","1. 🔄 **Interleaved Reasoning and Acting**: Fuses reasoning with actions for fluid updates.\n","\n","2. 🌍 **External Interactions**: Accesses external sources like Wikipedia for more info.\n","\n","3. 🧑‍🎨 **Human-like Problem Solving**: Offers easy-to-follow, logical trajectories.\n","\n","4. 🛠️ **Wide-Ranging Use Cases**: Ideal for varied tasks like QA, text games, web browsing.\n","\n","<figure>\n","  <img src=\"https://ar5iv.labs.arxiv.org/html/2210.03629/assets/x1.png\" alt=\"Image Description\" style=\"width:100%\">\n","  <figcaption>Comparison of four prompting methods, (a) Standard, (b) Chain-of-thought (CoT, Reason Only), (c) Act-only, and (d) (Reason+Act), solving a HotpotQA question. (2) Comparison of (a) Act-only and (b) prompting methods to solve an interactive AlfWorld task. In both domains, the initial prompts of in-context examples are ommitted, and only show task solving trajectories generated by the model (Act, Thought) and the environment (Obs).</figcaption>\n","  <a href=\"https://ar5iv.labs.arxiv.org/html/2210.03629\">Image Source</a>\n","</figure>\n","\n","\n","### 🌟 **Why ReAct Stands Out**\n","\n","1. **Balanced Emphasis on Reasoning & Acting**: Surpasses models limited to just reasoning or action.\n","\n","2. **Engages with External Data**: Pulls in real-time info for better accuracy.\n","\n","3. **Boosted Transparency**: Step-by-step approach clarifies decision-making.\n","\n","\n","# 🤖 **Overview of ReAct in LangChain**\n","\n","- ReAct is an agent type in LangChain.\n","\n","- It uses the ReAct framework to pick tools based on their descriptions.\n","\n","- Useful for chatbots, document question answering, etc.\n","\n","- Context-aware for better response generation.\n","\n","## 🚀 **Steps to Implement an Agent**\n","\n","1. ✍️ **Prompt Template Creation**: Guide the agent on tasks and output formatting.\n","\n","2. 🧩 **Output Parser Definition**: Convert LLM output into actionable responses.\n","\n","3. 🧠 **LLM Setup**: Choose the preferred language model.\n","\n","4. 🛑 **Stop Sequence Definition**: Instruct the LLM when to stop output.\n","\n","5. 🤹 **Agent Setup**: Merge LLM, prompt template, output parser, and stop sequence.\n","\n","6. 🏃 **Agent Usage**: Operate the agent using an executor with user inputs."],"metadata":{"id":"-sYuYFUvk-V6"}},{"cell_type":"code","source":["from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n","from langchain.prompts import StringPromptTemplate\n","from langchain.chains import LLMChain\n","from langchain.llms import OpenAI\n","from langchain.tools import DuckDuckGoSearchRun\n","from langchain import Wikipedia\n","from typing import List, Union\n","from langchain.schema import AgentAction, AgentFinish, OutputParserException\n","import re"],"metadata":{"id":"xkAmX2VCoTug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["search = DuckDuckGoSearchRun()\n","\n","search_tool = Tool(\n","    name=\"Current Event Search\",\n","    description = \"Useful for when you need to search about current events\",\n","    func=search.run,\n","    return_direct=True\n",")\n","\n","wiki = Wikipedia()\n","\n","wiki_tool = Tool(\n","    name=\"Noun Search\",\n","    description = \"Useful for when you need to search for information about nouns.\",\n","    func=wiki.search,\n","    return_direct=True\n",")\n","\n","tool_belt = [search_tool, wiki_tool]"],"metadata":{"id":"XOUua9vinI_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up the base template\n","template = \"\"\"Answer the following questions as best you can.\n","\n"," You have access to the following tools:\n","\n","{tools}\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","\n","Thought: you should always think about what to do\n","\n","Action: the action to take, should be one of [{tool_names}]\n","\n","Action Input: the input to the action\n","\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","\n","Thought: I now know the final answer\n","\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","\n","Question: {input}\n","\n","Thought: {agent_scratchpad}\n","\n","\"\"\""],"metadata":{"id":"ajkuMFWkoRvl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🔧 **Custom Prompt Template for LLM Agent**\n","\n","- Inherits from `StringPromptTemplate`.\n","\n","- `format` method key for prompt formatting.\n","\n","## 🛠️ **Inside the `format` Method**\n","\n","1. 📥 Receives keyword arguments (`**kwargs`).\n","\n","2. 🔄 Retrieves and formats intermediate steps `(AgentAction, Observation tuples)`.\n","\n","3. 🧠 Stores formatted steps in `thoughts` variable.\n","\n","4. 📝 Sets `agent_scratchpad` to `thoughts`.\n","\n","5. 🧩 Creates `tools` variable with tool names and descriptions.\n","\n","6. 🏷️ Forms `tool_names` by joining tool names.\n","\n","7. 🎨 Finalizes and returns the formatted template using keyword arguments."],"metadata":{"id":"rmQhMNbqo_oU"}},{"cell_type":"code","source":["# Set up a prompt template\n","class CustomPromptTemplate(StringPromptTemplate):\n","    # The template to use\n","    template: str\n","    # The list of tools available\n","    tools: List[Tool]\n","\n","    def format(self, **kwargs) -> str:\n","        # Get the intermediate steps (AgentAction, Observation tuples)\n","        # Format them in a particular way\n","        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n","        thoughts = \"\"\n","        for action, observation in intermediate_steps:\n","            thoughts += action.log\n","            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n","        # Set the agent_scratchpad variable to that value\n","        kwargs[\"agent_scratchpad\"] = thoughts\n","        # Create a tools variable from the list of tools provided\n","        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n","        # Create a list of tool names for the tools provided\n","        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n","        return self.template.format(**kwargs)"],"metadata":{"id":"c-l-O1mApBfd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = CustomPromptTemplate(\n","    template=template,\n","    tools=tool_belt,\n","    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n","    # This includes the `intermediate_steps` variable because that is needed\n","    input_variables=[\"input\", \"intermediate_steps\"]\n",")"],"metadata":{"id":"G7tMDNvvpCv2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🔍 **Creating a Custom Output Parser for LangChain Agent**\n","\n","- Inherits from `AgentOutputParser`.\n","\n","- The `parse` method is key for parsing LLM output.\n","\n","### 📝 **Inside the `parse` Method**\n","\n","1. 📩 Takes in LLM output as a string.\n","\n","2. 🔄 Decides between `AgentAction` and `AgentFinish`.\n","\n","3. 🎯 Checks for \"Final Answer:\" to trigger `AgentFinish`.\n","   - Extracts final answer if present.\n","\n","4. 🕵️‍♂️ If no finish trigger, uses regex to parse action and input.\n","\n","5. 🚀 Extracts action from regex's first group, and input from second.\n","\n","6. 📦 Returns `AgentAction` object with parsed action and input."],"metadata":{"id":"y1FKnF8PpEZ1"}},{"cell_type":"code","source":["class CustomOutputParser(AgentOutputParser):\n","\n","    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n","        # Check if agent should finish\n","        if \"Final Answer:\" in llm_output:\n","            return AgentFinish(\n","                # Return values is generally always a dictionary with a single `output` key\n","                # It is not recommended to try anything else at the moment :)\n","                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n","                log=llm_output,\n","            )\n","        # Parse out the action and action input\n","        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n","        match = re.search(regex, llm_output, re.DOTALL)\n","        if not match:\n","            raise OutputParserException(f\"Could not parse LLM output: `{llm_output}`\")\n","        action = match.group(1).strip()\n","        action_input = match.group(2)\n","        # Return the action and action input\n","        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"],"metadata":{"id":"guJvqfa3pMzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_parser = CustomOutputParser()"],"metadata":{"id":"fJUOu4bHpOda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = OpenAI(temperature=0)"],"metadata":{"id":"WpNWBQLqpQB7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🛑 **Defining the Stop Sequence for LLM Responses**\n","\n","- Tailored to the specific prompt and model.\n","\n","- Usually aligns with the token denoting the start of an Observation.\n","\n","- Prevents LLM from creating unintended observations.\n","\n","### 🤖 **Setting Up the Agent**\n","\n","1. 🔗 **Combine Key Components**: Merge the LLM, output parser, stop sequence.\n","\n","2. 🧰 **Include Allowed Tools**: Integrate tools permissible for the agent's use."],"metadata":{"id":"IOHA-RI2pRUP"}},{"cell_type":"code","source":["# LLM chain consisting of the LLM and a prompt\n","llm_chain = LLMChain(llm=llm, prompt=prompt)"],"metadata":{"id":"jMI6mvFipZtD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 🚀 **LLMSingleActionAgent in LangChain**\n","\n","- Designed for single actions based on input.\n","\n","- Utilizes a language model (LLM) for decision-making.\n","\n","### 📝 **How It Works**\n","\n","1. 📥 **Inputs**: Receives a prompt, tool list, and past steps.\n","\n","2. 🤖 **LLM Processing**: Analyzes prompt, context, and conversation state.\n","\n","3. 🧠 **Decision-Making**: Uses LLM to choose the next action.\n","\n","4. 🛠️ **Action Generation**: Determines tool to use and its input.\n","\n","5. 🎯 **Role as a Reasoning Engine**: Leverages LLM to make informed actions."],"metadata":{"id":"R51reCOtrspH"}},{"cell_type":"code","source":["tool_names = [tool.name for tool in tool_belt]\n","\n","agent = LLMSingleActionAgent(\n","    llm_chain=llm_chain,\n","    output_parser=output_parser,\n","    stop=[\"\\nObservation:\"],\n","    allowed_tools=tool_names\n",")"],"metadata":{"id":"fJXvzAsspcr8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent_executor = AgentExecutor.from_agent_and_tools(\n","    agent=agent,\n","    tools=tool_belt,\n","    verbose=True)\n","\n","agent_executor.run(\"Tell me about solar eclipses in 2023\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"4RB34Orepdsd","executionInfo":{"status":"ok","timestamp":1697194988968,"user_tz":300,"elapsed":2508,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"d1555059-0531-41b6-e2fe-b4d285887e39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mI should search for current events related to solar eclipses in 2023.\n","\n","Action: Current Event Search\n","\n","Action Input: solar eclipses in 2023\n","\u001b[0m\n","\n","Observation:\u001b[36;1m\u001b[1;3mBased on observations from several NASA missions, the map details the path of the Moon's shadow as it crosses the contiguous U.S. during the annular solar eclipse on October 14, 2023, and total solar eclipse on April 8, 2024. Oct. 12, 2023, 1:49 p.m. ET This weekend, millions of people will feast their eyes on the latest celestial marvel: an annular solar eclipse gracing the skies of the Western Hemisphere. The... The Saturday, Oct. 14, 2023, annular solar eclipse will cross North, Central, and South America. It will be visible in parts of the United States, Mexico, and many countries in South and Central America. NASA's Eclipse Explorer is an interactive map designed to enhance your eclipse-viewing experience. Don't Miss the 2023 Annular Solar Eclipse By Anne Buckle. Published 27-Sep-2023 On October 14, an annular solar eclipse will sweep across North and South America. The timeanddate team will be on the ground in Roswell, New Mexico, streaming it LIVE to you. When the Moon is far from the Earth, its size is too small to completely cover the Sun, thus an annular eclipse is observed, like what is expected on Oct. 14, 2023. When the Moon is close to the Earth, its larger size completely covers the Sun, causing a total eclipse, which will occur on April 8, 2024.\u001b[0m\n","\u001b[32;1m\u001b[1;3m\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Based on observations from several NASA missions, the map details the path of the Moon's shadow as it crosses the contiguous U.S. during the annular solar eclipse on October 14, 2023, and total solar eclipse on April 8, 2024. Oct. 12, 2023, 1:49 p.m. ET This weekend, millions of people will feast their eyes on the latest celestial marvel: an annular solar eclipse gracing the skies of the Western Hemisphere. The... The Saturday, Oct. 14, 2023, annular solar eclipse will cross North, Central, and South America. It will be visible in parts of the United States, Mexico, and many countries in South and Central America. NASA's Eclipse Explorer is an interactive map designed to enhance your eclipse-viewing experience. Don't Miss the 2023 Annular Solar Eclipse By Anne Buckle. Published 27-Sep-2023 On October 14, an annular solar eclipse will sweep across North and South America. The timeanddate team will be on the ground in Roswell, New Mexico, streaming it LIVE to you. When the Moon is far from the Earth, its size is too small to completely cover the Sun, thus an annular eclipse is observed, like what is expected on Oct. 14, 2023. When the Moon is close to the Earth, its larger size completely covers the Sun, causing a total eclipse, which will occur on April 8, 2024.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["# LangChain does all the work for you\n","\n","Using the same set of tools that were defined at the start of the notebook, you can also define a ReAct agent as follows:"],"metadata":{"id":"_I-wOB5vqZEh"}},{"cell_type":"code","source":["from langchain.agents import initialize_agent\n","from langchain.agents import AgentType\n","\n","# initialize agent with the tools, language model, and the type of agent\n","agent = initialize_agent(tool_belt,\n","                         llm,\n","                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n","                         verbose=True\n","                         )"],"metadata":{"id":"iwr8F7hhrMPA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(agent.agent.llm_chain.prompt.template)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11QiQgjcrQWU","executionInfo":{"status":"ok","timestamp":1697194932089,"user_tz":300,"elapsed":305,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"88c0ec5c-30c5-4cf7-8126-dadba2dbac2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Answer the following questions as best you can. You have access to the following tools:\n","\n","Current Event Search: Useful for when you need to search about current events\n","Noun Search: Useful for when you need to search for information about nouns.\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [Current Event Search, Noun Search]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","\n","Question: {input}\n","Thought:{agent_scratchpad}\n"]}]},{"cell_type":"code","source":["agent.run(\"Tell me about solar eclipses in 2023\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"id":"jYCk74mDrWqu","executionInfo":{"status":"ok","timestamp":1697194947001,"user_tz":300,"elapsed":2446,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"f86c7fe5-8e8f-4372-b311-cbfc8e04e9e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I need to find out information about solar eclipses in 2023\n","Action: Current Event Search\n","Action Input: Solar eclipses in 2023\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3mOct. 12, 2023, 1:49 p.m. ET This weekend, millions of people will feast their eyes on the latest celestial marvel: an annular solar eclipse gracing the skies of the Western Hemisphere. The... The Saturday, Oct. 14, 2023, annular solar eclipse will cross North, Central, and South America. It will be visible in parts of the United States, Mexico, and many countries in South and Central America. NASA's Eclipse Explorer is an interactive map designed to enhance your eclipse-viewing experience. Based on observations from several NASA missions, the map details the path of the Moon's shadow as it crosses the contiguous U.S. during the annular solar eclipse on October 14, 2023, and total solar eclipse on April 8, 2024. Watching a solar eclipse is a human experience, one in which people across the world and across time can share a moment of cosmic wonder, writes Don Lincoln. ... 2023 Link Copied! Ad Feedback ... USA TODAY Millions of Americans could be able to see a spectacular solar eclipse on Saturday, Oct. 14 as the moon passes in front of the sun, temporarily darkening the sky over a swath of the...\u001b[0m\n","\u001b[32;1m\u001b[1;3m\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Oct. 12, 2023, 1:49 p.m. ET This weekend, millions of people will feast their eyes on the latest celestial marvel: an annular solar eclipse gracing the skies of the Western Hemisphere. The... The Saturday, Oct. 14, 2023, annular solar eclipse will cross North, Central, and South America. It will be visible in parts of the United States, Mexico, and many countries in South and Central America. NASA's Eclipse Explorer is an interactive map designed to enhance your eclipse-viewing experience. Based on observations from several NASA missions, the map details the path of the Moon's shadow as it crosses the contiguous U.S. during the annular solar eclipse on October 14, 2023, and total solar eclipse on April 8, 2024. Watching a solar eclipse is a human experience, one in which people across the world and across time can share a moment of cosmic wonder, writes Don Lincoln. ... 2023 Link Copied! Ad Feedback ... USA TODAY Millions of Americans could be able to see a spectacular solar eclipse on Saturday, Oct. 14 as the moon passes in front of the sun, temporarily darkening the sky over a swath of the...\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}]}]}