{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLZDejBAkdC0RNUGMBnQTj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"63EtYTvnu1r4"},"outputs":[],"source":["%%capture\n","!pip install langchain openai tiktoken faiss-cpu"]},{"cell_type":"code","source":["import os\n","import getpass"],"metadata":{"id":"a15mWUKivAZL","executionInfo":{"status":"ok","timestamp":1695986032809,"user_tz":300,"elapsed":2,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjmRd93XvAmj","executionInfo":{"status":"ok","timestamp":1695986035191,"user_tz":300,"elapsed":2009,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"94baa229-d114-4dc7-e887-3921066cb291"},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Your OpenAI API Key:··········\n"]}]},{"cell_type":"code","source":["!wget -O \"golden_hymns_of_epictetus.txt\" https://www.gutenberg.org/cache/epub/871/pg871.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LrzoY-WvB4M","executionInfo":{"status":"ok","timestamp":1695984396457,"user_tz":300,"elapsed":682,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"1fcd48d2-513f-4293-be07-7c83764ba2d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-09-29 10:46:35--  https://www.gutenberg.org/cache/epub/871/pg871.txt\n","Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 152365 (149K) [text/plain]\n","Saving to: ‘golden_hymns_of_epictetus.txt’\n","\n","golden_hymns_of_epi 100%[===================>] 148.79K  --.-KB/s    in 0.1s    \n","\n","2023-09-29 10:46:36 (1021 KB/s) - ‘golden_hymns_of_epictetus.txt’ saved [152365/152365]\n","\n"]}]},{"cell_type":"code","source":["filename = \"/content/golden_hymns_of_epictetus.txt\"\n","\n","start_saving = False\n","stop_saving = False\n","lines_to_save = []\n","\n","with open(filename, 'r') as file:\n","    for line in file:\n","        if \"Are these the only works of Providence within us?\" in line:\n","            start_saving = True\n","        if \"*** END OF THE PROJECT GUTENBERG EBOOK THE GOLDEN SAYINGS OF EPICTETUS, WITH THE HYMN OF CLEANTHES ***\" in line:\n","            stop_saving = True\n","            break\n","        if start_saving and not stop_saving:\n","            lines_to_save.append(line)\n","\n","# Write the stored lines back to the file\n","with open(filename, 'w') as file:\n","    for line in lines_to_save:\n","        file.write(line)"],"metadata":{"id":"0zQORqzdwHU3","executionInfo":{"status":"ok","timestamp":1695985353252,"user_tz":300,"elapsed":150,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["word_count = 0\n","\n","with open(filename, 'r') as file:\n","    for line in file:\n","        words = line.split()\n","        word_count += len(words)\n","\n","print(f\"The total number of words in the file is: {word_count}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-c2oiPNTwNKk","executionInfo":{"status":"ok","timestamp":1695985355389,"user_tz":300,"elapsed":106,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"cca9d3b0-c8d3-4112-fdb0-058d08fa503f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["The total number of words in the file is: 23503\n"]}]},{"cell_type":"markdown","source":["# Retrieval Overview\n","\n","Retrieval in LangChain refers to the process of fetching and retrieving relevant data or documents from external sources.\n","\n","It is a crucial step in many language model applications, especially in Retrieval Augmented Generation (RAG) tasks.\n","\n","Retrieval is useful because it allows you to incorporate external data into your language model, providing additional context and information that may not be present in the model's training data.\n","\n","By retrieving relevant documents, you can enhance the generation process and improve the quality and relevance of the generated responses.\n","\n","# Document Loaders\n","\n","Document loaders in LangChain are used to load data from various sources as Document objects.\n","\n","A Document is a piece of text with associated metadata.\n","\n","Document loaders provide a convenient way to fetch data from different sources such as text files, web pages, or even transcripts of videos.\n","\n","## Text loader\n","\n","Ths is the simplest loader. It reads in a file as text and places it all into one Document"],"metadata":{"id":"NuzRSeWYwOn5"}},{"cell_type":"code","source":["from langchain.document_loaders import TextLoader\n","loader = TextLoader(\"/content/golden_hymns_of_epictetus.txt\")\n","golden_sayings = loader.load()"],"metadata":{"id":"ECz7yiNBwZKi","executionInfo":{"status":"ok","timestamp":1695985363499,"user_tz":300,"elapsed":759,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["type(golden_sayings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7ghrb20wbXO","executionInfo":{"status":"ok","timestamp":1695984403831,"user_tz":300,"elapsed":100,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"58223e42-b56a-4d06-bb13-0db34b8a8483"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["type(golden_sayings[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cHJNMIwdwhAv","executionInfo":{"status":"ok","timestamp":1695984404606,"user_tz":300,"elapsed":2,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"7ad3770d-b465-4e97-e9c3-ee1d50a85db3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["langchain.schema.document.Document"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# There are SO MANY document loaders in LangChain\n","\n","I won't go every single one in this notebook. But, you can check out [the documentation](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/document_loaders) to see jusy how many are available to you.\n","\n","They all follow the same pattern:\n","\n","\n","1) Import the `DirectoryLoader` class from the `langchain.document_loaders module`.\n","\n","2) Create an instance of the `DirectoryLoader` class, providing the path to the directory as the argument.\n","\n","3) Use the `load()` method of the `DirectoryLoader` instance to load all the files in the directory and convert them into LangChain's Document format.\n","\n","# Document transformers\n","\n","Document transformers in LangChain are used to manipulate and transform documents to better suit your application's needs.\n","\n","You need document transformers when you want to perform operations on your documents, such as splitting a long document into smaller chunks that can fit into your model's context window.\n","\n","# Text splitters\n","\n","Text splitters in LangChain are used to split long pieces of text into smaller, semantically meaningful chunks.\n","\n","They are particularly useful when you want to keep related pieces of text together or when you need to process text in smaller segments.\n","\n","### At a high level, text splitters work as following:\n","\n","1) Split the text up into small, semantically meaningful chunks (often sentences).\n","\n","2) Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n","\n","3) Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n","\n","### The default recommended text splitter is the `RecursiveCharacterTextSplitter`.\n","\n","`RecursiveCharacterTextSplitter` is like a smart text splitting tool.\n","\n","Instead of just blindly splitting text by a single separator, it tries multiple separators in a specific order, and if a piece of text is too big, it'll try to split it again using a different separator. For code or text in various programming languages, it has predefined lists of separators that make sense for each language, ensuring the text is split in a logical way.\n","\n","\n","It tries to create chunks based on splitting on the first character, but if any chunks are too large it then moves onto the next character, and so forth.\n","\n","### The Splitting Process:\n","\n"," - The method tries to find the best separator from the list to split the given text.\n","\n"," - Once a separator is found, it splits the text.\n","\n"," - If any of the resulting chunks are too big, it'll try to split that chunk again using the next separator in the list.\n","\n"," - This process is recursive, which means it keeps trying to split chunks until they're small enough or there are no more separators to try.\n","\n","• `length_function`: how the length of chunks is calculated. Defaults to just counting number of characters, but it's pretty common to pass a token counter here.\n","\n","• `chunk_size`: the maximum size of your chunks (as measured by the length function).\n","\n","• `chunk_overlap`: the maximum overlap between chunks. It can be nice to have some overlap to maintain some continuity between chunks (e.g. do a sliding window).\n","\n","• `add_start_index`: whether to include the starting position of each chunk within the original document in the metadata."],"metadata":{"id":"onMQEXT7wjMc"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,\n","    chunk_overlap = 50,\n","    length_function = len,\n","    add_start_index = True\n",")"],"metadata":{"id":"4gItthdswrME","executionInfo":{"status":"ok","timestamp":1695985384897,"user_tz":300,"elapsed":186,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["texts = text_splitter.split_documents(golden_sayings)"],"metadata":{"id":"E00tIW5ox0n9","executionInfo":{"status":"ok","timestamp":1695985586043,"user_tz":300,"elapsed":93,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(texts[0])\n","print(texts[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mf0Nj7YMyJjg","executionInfo":{"status":"ok","timestamp":1695985587061,"user_tz":300,"elapsed":92,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"c9791f54-8585-46f6-bc3f-b510d05fbf94"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["page_content='Are these the only works of Providence within us? What words suffice to\\npraise or set them forth? Had we but understanding, should we ever\\ncease hymning and blessing the Divine Power, both openly and in secret,\\nand telling of His gracious gifts? Whether digging or ploughing or\\neating, should we not sing the hymn to God:—\\n\\n_Great is God_, for that He hath given us such instruments to till the\\nground withal:\\n\\n\\n_Great is God_, for that He hath given us hands and the power of\\nswallowing and digesting; of unconsciously growing and breathing while\\nwe sleep!\\n\\n\\nThus should we ever have sung; yea and this, the grandest and divinest\\nhymn of all:—\\n\\n_Great is God_, for that He hath given us a mind to apprehend these\\nthings, and duly to use them!' metadata={'source': '/content/golden_hymns_of_epictetus.txt', 'start_index': 0}\n","page_content='What then! seeing that most of you are blinded, should there not be\\nsome one to fill this place, and sing the hymn to God on behalf of all\\nmen? What else can I that am old and lame do but sing to God? Were I a\\nnightingale, I should do after the manner of a nightingale. Were I a\\nswan, I should do after the manner of a swan. But now, since I am a\\nreasonable being, I must sing to God: that is my work: I do it, nor\\nwill I desert this my post, as long as it is granted me to hold it; and\\nupon you too I call to join in this self-same hymn.\\n\\nII\\n\\nHow then do men act? As though one returning to his country who had\\nsojourned for the night in a fair inn, should be so captivated thereby\\nas to take up his abode there.\\n\\n“Friend, thou hast forgotten thine intention! This was not thy\\ndestination, but only lay on the way thither.”\\n\\n“Nay, but it is a proper place.”' metadata={'source': '/content/golden_hymns_of_epictetus.txt', 'start_index': 746}\n"]}]},{"cell_type":"markdown","source":["# Text Embeddings\n","\n","Text embedding models for retrieval in LangChain are used to represent text documents in a high-dimensional vector space, where the similarity between vectors corresponds to the semantic similarity between the corresponding documents.\n","\n","These models capture the semantic meaning of text and allow for efficient retrieval of similar documents based on their embeddings."],"metadata":{"id":"OsF1E9sUyhL-"}},{"cell_type":"code","source":["from langchain.embeddings import OpenAIEmbeddings\n","embeddings = OpenAIEmbeddings()"],"metadata":{"id":"cIvaIq2xywMw","executionInfo":{"status":"ok","timestamp":1695986040066,"user_tz":300,"elapsed":107,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["To construct a vector store retriever, you first need to load the documents using a document loader.\n","\n","Then, you can split the documents into smaller chunks using a text splitter. Next, you can generate vector embeddings for the text chunks using an embedding model like OpenAIEmbeddings. Finally, you can create a vector store using the generated embeddings.\n","\n","Once the vector store is constructed, you can use it as a retriever to query the texts."],"metadata":{"id":"Wn6N5TfLq1Js"}},{"cell_type":"code","source":["from langchain.vectorstores import FAISS\n","vectorstore = FAISS.from_documents(documents=texts, embedding=OpenAIEmbeddings())"],"metadata":{"id":"tMTITqujy-Ny","executionInfo":{"status":"ok","timestamp":1695986316802,"user_tz":300,"elapsed":3160,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# Retrieve\n","\n","Once the vector store is constructed, you can use it as a retriever to query the texts.\n","\n","The vector store retriever supports various search methods, including similarity search and maximum marginal relevance search.\n","\n","You can also set a similarity score threshold or specify the number of top documents to retrieve.\n","\n","Below you will use the `similarity_search` method of the vector store.\n","\n","In simpler terms, think of this function as a search tool. You give it a piece of text, tell it how many results you want, and it returns a list of documents that are most similar to your given text.\n","\n","If you have specific requirements, like only wanting documents from a certain author, you can use the filter option to specify that."],"metadata":{"id":"Ukx_BoFuqWgh"}},{"cell_type":"code","source":["query = \"How can I practice mindfulness if I am always so busy and distracted?\"\n","\n","vectorstore.similarity_search(query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoBei3GpskW8","executionInfo":{"status":"ok","timestamp":1695986940761,"user_tz":300,"elapsed":426,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"dd47e3cb-f273-4dfd-cbe8-d46f47587aad"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='One who has had fever, even when it has left him, is not in the same\\ncondition of health as before, unless indeed his cure is complete.\\nSomething of the same sort is true also of diseases of the mind.\\nBehind, there remains a legacy of traces and blisters: and unless these\\nare effectually erased, subsequent blows on the same spot will produce\\nno longer mere blisters, but sores. If you do not wish to be prone to\\nanger, do not feed the habit; give it nothing which may tend its\\nincrease. At first, keep quiet and count the days when you were not\\nangry: “I used to be angry every day, then every other day: next every\\ntwo, next every three days!” and if you succeed in passing thirty days,\\nsacrifice to the Gods in thanksgiving.\\n\\nLXXVI\\n\\nHow then may this be attained?—Resolve, now if never before, to approve\\nthyself to thyself; resolve to show thyself fair in God’s sight; long\\nto be pure with thine own pure self and God!\\n\\nLXXVII', metadata={'source': '/content/golden_hymns_of_epictetus.txt', 'start_index': 50909}),\n"," Document(page_content='Friend, bethink you first what it is you would do, and then what your\\nown nature is able to bear. Would you be a wrestler, consider your\\nshoulders, your thighs, your loins—not all men are formed to the same\\nend. Think you to be a philosopher while acting as you do? think you go\\non thus eating, thus drinking, giving way in like manner to wrath and\\nto displeasure? Nay, you must watch, you must labour; overcome certain\\ndesires; quit your familiar friends, submit to be despised by your\\nslave, to be held in derision by them that meet you, to take the lower\\nplace in all things, in office, in positions of authority, in courts of\\nlaw.\\n\\nWeigh these things fully, and then, if you will, lay to your hand; if\\nas the price of these things you would gain Freedom, Tranquillity, and\\npassionless Serenity.\\n\\nCV\\n\\nHe that hath no musical instruction is a child in Music; he that hath\\nno letters is a child in Learning; he that is untaught is a child in\\nLife.\\n\\nCVI', metadata={'source': '/content/golden_hymns_of_epictetus.txt', 'start_index': 71993}),\n"," Document(page_content='CXIV\\n\\nHow can it be that one who hath nothing, neither raiment, nor house,\\nnor home, nor bodily tendance, nor servant, nor city, should yet live\\ntranquil and contented? Behold God hath sent you a man to show you in\\nact and deed that it may be so. Behold me! I have neither house nor\\npossessions nor servants: the ground is my couch; I have no wife, no\\nchildren, no shelter—nothing but earth and sky, and one poor cloak. And\\nwhat lack I yet? am I not untouched by sorrow, by fear? am I not free?\\n. . . when have I laid anything to the charge of God or Man? when have\\nI accused any? hath any of you seen me with a sorrowful countenance?\\nAnd in what wise treat I those of whom you stand in fear and awe? Is it\\nnot as slaves? Who when he seeth me doth not think that he beholdeth\\nhis Master and his King?\\n\\nCXV\\n\\nGive thyself more diligently to reflection: know thyself: take counsel\\nwith the Godhead: without God put thine hand unto nothing!\\n\\nCXVI', metadata={'source': '/content/golden_hymns_of_epictetus.txt', 'start_index': 78345}),\n"," Document(page_content='What then am I to say to you? “Help me in this matter!” you cry. Ah,\\nfor that I have no rule! And neither did you, if that was your object,\\ncome to me as a philosopher, but as you might have gone to a\\nherb-seller or a cobbler.—“What do philosophers have rules for,\\nthen?”—Why, that whatever may betide, our ruling faculty may be as\\nNature would have it, and so remain. Think you this a small matter? Not\\nso! but the greatest thing there is. Well, does it need but a short\\ntime? Can it be grasped by a passer-by?—grasp it, if you can!', metadata={'source': '/content/golden_hymns_of_epictetus.txt', 'start_index': 64380})]"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["# Generate"],"metadata":{"id":"P5quhR5asvju"}},{"cell_type":"code","source":["from langchain.chains import RetrievalQA\n","from langchain.prompts import PromptTemplate\n","from langchain.chat_models import ChatOpenAI\n","\n","template = \"\"\"\n","\n","Use the following pieces of context to answer the question at the end.\n","\n","If you don't know the answer, just say 'Ah snap homie, I ain't gonna front. I don't know.`, don't try to make up an answer.\n","\n","Use three sentences maximum, relevant analogies, and keep the answer as concise as possible.\n","\n","Use the active voice, and speak directly to the reader using concise language.\n","{context}\n","\n","Question: {question}\n","\n","Helpful Answer:\n","\n","\"\"\"\n","\n","QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectorstore.as_retriever(),\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")\n","\n","\n","query = \"What do grief, fear, envy, and desire stem from?\"\n","\n","result = qa_chain({\"query\": query})\n","\n","result[\"result\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"BwZKC0Yzs3ta","executionInfo":{"status":"ok","timestamp":1695987105885,"user_tz":300,"elapsed":3737,"user":{"displayName":"Harpreet Sahota","userId":"04881662502078178826"}},"outputId":"90c624a6-18ce-4262-8389-9503a22e8641"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Grief, fear, envy, and desire stem from the mind's attachment to worldly desires and emotions, such as Fear, Desire, Envy, Malignity, Avarice, Effeminacy, and Intemperance. These negative emotions can only be cast out by looking to God alone, fixing one's affections on Him, and consecrating oneself to His commands.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]}]}